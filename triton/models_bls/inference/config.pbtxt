name: "inference"
backend: "onnxruntime"
default_model_filename: "model.onnx"
max_batch_size: 8
dynamic_batching { }

input [
  {
    name: "input_ids"
    data_type: TYPE_INT64
    dims: [ -1 ]
  },
  {
    name: "token_type_ids"
    data_type: TYPE_INT64
    dims: [ -1 ]
  },
  {
    name: "attention_mask"
    data_type: TYPE_INT64
    dims: [ -1 ]
  }
]

output [
  {
    name: "logits"
    data_type: TYPE_FP32
    dims: [ 28 ]
  }
]

version_policy: { all { }}
instance_group {
  count: 1
  kind: KIND_CPU
}
